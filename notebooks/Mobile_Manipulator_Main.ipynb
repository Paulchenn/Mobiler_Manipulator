{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project: Mobile Manipulator Planning\n",
    "**Course:** Roboterprogrammierung (Winter 25/26)  \n",
    "**Institution:** Hochschule Karlsruhe - University of Applied Sciences (HKA)  \n",
    "\n",
    "## üéØ Objective\n",
    "This notebook implements and evaluates a **Collision Checker** for a planar mobile robot consisting of a movable base and a 2-DOF rotatory arm. The goal is to benchmark path planning algorithms (LazyPRM, VisibilityPRM, etc.) in various environments while handling self-collisions and obstacle avoidance.\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "* **Mobile Base:** Free-form shape definition (2D).\n",
    "* **Manipulator:** Configurable arm segments (Length, Thickness, Joint Limits).\n",
    "* **Collision Detection:** Custom implementation handling Base-Obstacle, Arm-Obstacle, and Self-Collision (Arm-Base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-reload modules when source code changes ---\n",
    "# This is crucial so you don't have to restart the kernel when editing src/ files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import copy\n",
    "\n",
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches # Needed for the custom legend\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "from ipywidgets import interact, FloatSlider, Dropdown, Layout, Checkbox, Button, VBox, HBox, Output\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "\n",
    "# --- Add 'src' directory to path to import modules ---\n",
    "# Assuming this notebook is in the 'notebooks/' folder and source is in 'src/'\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "# IMPORTANT: To ensure that it finds its dependencies (IPTestSuite),\n",
    "# we explicitly add the src folder to the path.\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "# IMPORTANT: To ensure that IPLazyPRM finds its dependencies (IPPerfMonitor),\n",
    "# we explicitly add the src/planners folder to the path.\n",
    "planners_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src', 'planners'))\n",
    "if planners_path not in sys.path:\n",
    "    sys.path.append(planners_path)\n",
    "\n",
    "print(f\"Project Root added to path: {project_root}\")\n",
    "print(f\"Planner Root added to path: {planners_path}\")\n",
    "\n",
    "# --- Import Test Suite ---\n",
    "from IPTestSuite import * #benchList, START, GOAL, LIMITS, ROBOT_BASE_SHAPE, ROBOT_ARM_CONFIG, ARM_OFFSET\n",
    "\n",
    "# --- Import Animator ---\n",
    "from IPAnimator import IPAnimator\n",
    "\n",
    "# --- Import Performance Monitor ---\n",
    "from IPPerfMonitor import IPPerfMonitor\n",
    "\n",
    "# --- Import Result Collection ---\n",
    "from IPResultCollection import ResultCollection\n",
    "\n",
    "# --- Import Batch Evaluator\n",
    "from IPBatchEvaluator import BatchEvaluator\n",
    "\n",
    "# --- Import Planner Runner ---\n",
    "from IPMultiGoalPlannerRunner import MultiGoalPlannerRunner\n",
    "\n",
    "# --- Import Visualisator ---\n",
    "from IPBenchmarkPlotter import BenchmarkPlotter\n",
    "from IPBatchPlotter import BatchPlotter\n",
    "from IPSingleRunPlotter import SingleRunPlotter\n",
    "\n",
    "# --- Import Planners ---\n",
    "# Visibility PRM\n",
    "from IPVisibilityPRM import VisPRM as VisPRM\n",
    "from IPVisibilityPRM_multiGoal import VisPRM as VisPRM_multiGoal\n",
    "from IPVISVisibilityPRM import visibilityPRMVisualize\n",
    "# Lazy PRM\n",
    "from IPLazyPRM import LazyPRM as LazyPRM\n",
    "from IPLazyPRM_multiGoal import LazyPRM as LazyPRM_multiGoal\n",
    "from IPVISLazyPRM import lazyPRMVisualize\n",
    "\n",
    "folderPath2save = \"/Users/glaserpaul/Documents/010_HKA_RKIM/Mobiler_Manipulator/runs/20251228_SelfCheck\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Planner Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plannerFactory = dict()\n",
    "\n",
    "# --- Visibility PRM ---\n",
    "# Strategie: VisPRM verwirft Knoten, die \"sichtbar\" sind. In 5D ist das Risiko hoch, \n",
    "# dass wir wichtige Verbindungsknoten zu fr√ºh verwerfen oder gar nicht finden.\n",
    "# Daher: Sehr hohes 'ntry', damit er oft genug w√ºrfelt, um die \"Guards\" in den engen Passagen zu finden.\n",
    "visbilityConfig = dict()\n",
    "visbilityConfig[\"ntry\"] = 500  # Empfehlung: 400 - 800 f√ºr 5D\n",
    "plannerFactory[\"visibilityPRM\"] = [VisPRM, visbilityConfig, visibilityPRMVisualize]\n",
    "\n",
    "# --- Lazy PRM ---\n",
    "# Strategie: \"Shoot first, ask questions later\". \n",
    "# Wir fluten den Raum mit Knoten.\n",
    "lazyConfig = dict()\n",
    "lazyConfig[\"initialRoadmapSize\"] = 500  # Startet mit vielen Knoten f√ºr gute Abdeckung\n",
    "lazyConfig[\"updateRoadmapSize\"]  = 200  # Wenn kein Pfad, leg ordentlich nach\n",
    "lazyConfig[\"kNearest\"] = 12             # 5D braucht mehr Nachbarn f√ºr stabile Pfade (Standard 2D ist oft 5-6)\n",
    "lazyConfig[\"maxIterations\"] = 15        # Gib ihm Zeit, den Graphen zu vergr√∂√üern, falls es eng wird\n",
    "plannerFactory[\"lazyPRM\"] = [LazyPRM, lazyConfig, lazyPRMVisualize]\n",
    "\n",
    "print(\"Planner initialization successfull. Following Planners loaded:\")\n",
    "plannerNames = []\n",
    "for planner in plannerFactory:\n",
    "    plannerNames.append(planner)\n",
    "    print(f\"    {planner}\")\n",
    "\n",
    "print(\"Benchmarks initialization successfull. Following Benchmarks loaded:\")\n",
    "benchNames = []\n",
    "for benchmark in benchList:\n",
    "    benchNames.append(benchmark.name)\n",
    "    print(f\"    {benchmark.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "# Namen f√ºr das Dropdown-Men√º extrahieren\n",
    "bench_names = [b.name for b in benchList]\n",
    "\n",
    "def interact_bench_suite(bench_name, x, y, theta, q1, q2, attach_obj):\n",
    "    \"\"\"\n",
    "    Interaktive Steuerung des Roboters in einer ausgew√§hlten Benchmark-Umgebung.\n",
    "    \"\"\"\n",
    "    plt.close('all')\n",
    "\n",
    "    # 1. Den passenden Benchmark und CollisionChecker ausw√§hlen\n",
    "    # Wir suchen das Benchmark-Objekt anhand des Namens\n",
    "    current_bench = next(b for b in benchList if b.name == bench_name)\n",
    "    cc = current_bench.collisionChecker\n",
    "\n",
    "    # --- NEU: Objekt anh√§ngen/abh√§ngen basierend auf Checkbox ---\n",
    "    # Wir greifen auf das globale PICK_OBJECT zu (oder current_bench.objectShape)\n",
    "    if attach_obj:\n",
    "        # Falls PICK_OBJECT nicht global verf√ºgbar ist, nutze current_bench.objectShape\n",
    "        obj_shape = cc.get_object_shape()\n",
    "        cc.attach_object(obj_shape)\n",
    "    else:\n",
    "        cc.detach_object()\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # 3. Konfiguration erstellen (Numpy Array!)\n",
    "    config = np.array([x, y, theta, q1, q2], dtype=float)\n",
    "\n",
    "    # 4. Kollision pr√ºfen\n",
    "    try:\n",
    "        if hasattr(cc, 'pointInCollision'):\n",
    "            is_collision = cc.pointInCollision(config)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler Collision Check: {e}\")\n",
    "        is_collision = True\n",
    "\n",
    "    # 5. Zeichnen\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        \n",
    "        # --- A. Hindernisse und aktueller Roboter ---\n",
    "        cc.drawObstacles(ax)\n",
    "        cc.drawRobot(config, ax, alpha=1)\n",
    "        \n",
    "        # --- B. Start und Ziel des Benchmarks einblenden (Hilfreich!) ---\n",
    "        # Startkonfiguration (Gr√ºn gestrichelt)\n",
    "        start_conf = np.array(current_bench.startList[0])\n",
    "        # print(start_conf)\n",
    "        # print(start_conf)\n",
    "        geo_start = cc.get_robot_geometry(start_conf)\n",
    "        ax.plot(*geo_start['base'].exterior.xy, color='green', linestyle='--', alpha=0.5, label='Start Base')\n",
    "        \n",
    "        # Zielkonfiguration (Rot gestrichelt)\n",
    "        for goals in current_bench.goalList:\n",
    "            # goalList Eintr√§ge k√∂nnen Tupel (Pos, Action) oder Listen (Pos) sein\n",
    "            # Wir extrahieren sicherheitshalber nur die Koordinate\n",
    "            goal_raw = goals[0] # if isinstance(goals, (tuple, list)) and len(goals) == 2 and isinstance(goals[1], str) else goals\n",
    "            \n",
    "            goal_conf = np.array(goal_raw)\n",
    "            # print(goal_conf)\n",
    "            \n",
    "            # Farbe bestimmen (letztes Ziel rot, Zwischenziele lila)\n",
    "            # Hinweis: Der Vergleich von Arrays ben√∂tigt .all()\n",
    "            last_goal_raw = current_bench.goalList[-1]\n",
    "            last_goal_coords = last_goal_raw[0] if isinstance(last_goal_raw, (tuple, list)) and len(last_goal_raw) == 2 and isinstance(last_goal_raw[1], str) else last_goal_raw\n",
    "            \n",
    "            if np.allclose(goal_conf, last_goal_coords):\n",
    "                goal_color = 'red'\n",
    "            else:\n",
    "                goal_color = 'purple'\n",
    "                \n",
    "            geo_goal = cc.get_robot_geometry(goal_conf)\n",
    "            ax.plot(*geo_goal['base'].exterior.xy, color=goal_color, linestyle='--', alpha=0.5, label='Goal Base')\n",
    "        \n",
    "        # Arme f√ºr Start/Ziel auch andeuten (optional, hier nur Basis der √úbersicht halber)\n",
    "\n",
    "        # --- C. Plot Settings ---\n",
    "        ax.set_xlim(LIMITS[0]) # Setzt min_x und max_x\n",
    "        ax.set_ylim(LIMITS[1]) # Setzt min_y und max_y\n",
    "        ax.set_aspect('equal') # Verhindert Verzerrung (Kreise bleiben rund)\n",
    "\n",
    "        ax.axis('on') \n",
    "        \n",
    "        # Major Ticks auf 1.0\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "        \n",
    "        # Minor Ticks auf 0.5\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.5))\n",
    "        ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.5))\n",
    "        \n",
    "        # Gitter zeichnen\n",
    "        ax.grid(which='major', alpha=0.5, color='gray', linestyle='-')\n",
    "        ax.grid(which='minor', alpha=0.2, color='gray', linestyle='--')\n",
    "        \n",
    "        # Labels anzeigen\n",
    "        ax.tick_params(axis='both', which='both', labelbottom=True, labelleft=True)\n",
    "\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        # Titel & Status\n",
    "        status_color = 'red' if is_collision else 'green'\n",
    "        status_text = \"COLLISION!\" if is_collision else \"FREE\"\n",
    "        attach_text = \" [HOLDING OBJECT]\" if attach_obj else \"\"\n",
    "        \n",
    "        ax.set_title(f\"Scenario: {bench_name}{attach_text} \\nStatus: {status_text} | Self check: {SELF_CHECK}\", \n",
    "                     color=status_color, fontweight='bold', fontsize=14)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Zeichnen: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "# --- Widgets erstellen ---\n",
    "point_to_show = START[0]#[0]\n",
    "interact(interact_bench_suite, \n",
    "         bench_name=Dropdown(options=bench_names, description='Scenario:'),\n",
    "         attach_obj=Checkbox(value=False, description='Attach Object'),\n",
    "         x=FloatSlider(min=LIMITS[0][0], max=LIMITS[0][1], step=0.1, value=point_to_show[0]),\n",
    "         y=FloatSlider(min=LIMITS[1][0], max=LIMITS[1][1], step=0.1, value=point_to_show[1]),\n",
    "         theta=FloatSlider(min=LIMITS[2][0], max=LIMITS[2][1], step=0.01, value=point_to_show[2], description='Base Theta'),\n",
    "         # Die Limits f√ºr die Gelenke holen wir direkt aus der zentralen Config\n",
    "         q1=FloatSlider(min=LIMITS[3][0], max=LIMITS[3][1], step=0.01, value=point_to_show[3], description='Joint 1'),\n",
    "         q2=FloatSlider(min=LIMITS[4][0], max=LIMITS[4][1], step=0.01, value=point_to_show[4], description='Joint 2')\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultList = list()\n",
    "plotList = []\n",
    "\n",
    "for key, producer in list(plannerFactory.items()):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{key}:\")\n",
    "    for benchmark in benchList:\n",
    "        print(f\"    Running Benchmark {benchmark.name}\")\n",
    "        \n",
    "        # Planner instanziieren\n",
    "        planner = producer[0](benchmark.collisionChecker)\n",
    "        config = producer[1]\n",
    "        \n",
    "        IPPerfMonitor.clearData()\n",
    "        \n",
    "        try:\n",
    "            # --- Hier passiert jetzt die Magie in einer Zeile ---\n",
    "            full_path, action_events, status = MultiGoalPlannerRunner.run_benchmark(planner, benchmark, config)\n",
    "            \n",
    "            # Ergebnis speichern\n",
    "            resultList.append(ResultCollection(\n",
    "                plannerFactoryName=key,\n",
    "                planner=planner, # Der Planner wurde in run_benchmark manipuliert (Graph, Edges)\n",
    "                benchmark=benchmark,\n",
    "                solution=full_path, \n",
    "                actions=action_events,\n",
    "                status=status,\n",
    "                perfDataFrame=IPPerfMonitor.dataFrame()\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if status['success']:\n",
    "                # plotList.append(1) # Gr√ºn in der Visualisierung\n",
    "                pass\n",
    "            else:\n",
    "                # Wir plotten es trotzdem (damit man sieht wie weit er kam), markieren es aber\n",
    "                print(f\"        PARTIAL PATH: {status['fail_reason']}\")\n",
    "            plotList.append(1) # 1 = Plotten, aber wir wissen es ist unvollst√§ndig\n",
    "            \n",
    "        except Exception as e:\n",
    "            plotList.append(0)\n",
    "            print(\"        PLANNING ERROR:\", e)\n",
    "            # Optional: Traceback f√ºr Details\n",
    "            # import traceback\n",
    "            # traceback.print_exc()\n",
    "            pass\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausf√ºhren\n",
    "# plannerNames und benchNames hast du ja oben im Notebook definiert\n",
    "SingleRunPlotter.visualize_and_save(\n",
    "    resultList, \n",
    "    plotList, \n",
    "    plannerNames, \n",
    "    benchNames, \n",
    "    plannerFactory, \n",
    "    save_plots=True,       # Auf True setzen zum Speichern\n",
    "    output_dir=folderPath2save # Ordnername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Limits (falls nicht global vorhanden)\n",
    "try:\n",
    "    anim_limits = LIMITS\n",
    "except NameError:\n",
    "    anim_limits = (-10, 25)\n",
    "\n",
    "# UI erstellen und anzeigen\n",
    "ui = IPAnimator.create_interactive_viewer(plannerFactory, resultList, limits=anim_limits)\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufrufen\n",
    "BenchmarkPlotter.visualize(\n",
    "    resultList,\n",
    "    benchList,\n",
    "    save_plots=True,       # Auf True setzen zum Speichern\n",
    "    output_dir=folderPath2save # Ordnername\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KONFIGURATION ---\n",
    "NUMBER_OF_RUNS = 10  # Wie oft soll jeder Test wiederholt werden?\n",
    "\n",
    "print(f\"Starting Evaluation with {NUMBER_OF_RUNS} runs per scenario...\")\n",
    "\n",
    "# Batch starten\n",
    "df_results = BatchEvaluator.run_experiment(plannerFactory, benchList, num_runs=NUMBER_OF_RUNS)\n",
    "\n",
    "print(\"Evaluation finished.\")\n",
    "# Kurzer Blick auf die Daten (erste 5 Zeilen)\n",
    "display(df_results.head())\n",
    "\n",
    "# Optional: Speichern als CSV, damit die Daten nicht verloren gehen\n",
    "# df_results.to_csv(f\"benchmark_results_{NUMBER_OF_RUNS}runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung starten\n",
    "BatchPlotter.visualize(\n",
    "    df_results,\n",
    "    save_plots=True,\n",
    "    output_dir=folderPath2save\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
